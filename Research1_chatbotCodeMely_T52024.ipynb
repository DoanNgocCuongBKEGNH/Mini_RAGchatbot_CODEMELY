{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "basic RAG system . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7gb28rrWZNF"
      },
      "source": [
        "- Quan sát 100-10-1 (Chọn lọc được các Ref phù hợp từ hàng ngàn nguồn)\n",
        "- Phân tích - Đúc kết\n",
        "\n",
        "- Reference:\n",
        "\n",
        "    1. [Step-by-Step Guide to Building a RAG LLM App with LLamA2 and LLaMAindex](https://youtu.be/f-AXdiCyiT8?feature=shared)\n",
        "    \n",
        "    2. [MicrosoftPhi2RAG](https://www.youtube.com/watch?v=SQYLAfQkQRw&pp=ygUTUkFHIHdpdGggUGhpIG1vZGVsIA%3D%3D)\n",
        "    \n",
        "    3. LLM(Llama-3 trên: [Reliable, fully local RAG agents with LLaMA3](https://www.youtube.com/watch?v=-ROS6gfYIts)) - 20/04/2024\n",
        "    hoặc: trên LightingAI (https://lightning.ai/lightning-ai/studios/rag-using-llama-3-by-meta-ai?utm_source=akshay)\n",
        "    \n",
        "    4. Quantization: LLM to GGUF format: https://youtu.be/wxQgGK5K0rE?feature=shared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dZgW2auzeBI"
      },
      "source": [
        "## 0. Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI5ETwyu0Kc4"
      },
      "source": [
        "- **llama_index**:\n",
        "  - Designed for vector index creation and management.\n",
        "  - Supports similarity searches and related applications in machine learning, NLP, and AI.\n",
        "  - Integrates with deep learning models, especially from Hugging Face, to optimize data querying and information extraction.\n",
        "\n",
        "\n",
        "\n",
        "- **llama_index**:\n",
        "  - Được thiết kế để tạo và quản lý chỉ mục vector.\n",
        "  - Hỗ trợ tìm kiếm tương đồng và các ứng dụng liên quan trong machine learning, NLP, AI.\n",
        "  - Tích hợp với các mô hình deep learning, đặc biệt là từ Hugging Face để tối ưu hóa việc truy vấn dữ liệu và trích xuất thông tin.\n",
        "\n",
        "- **langchain**:\n",
        "  - Appears to facilitate the integration of embedding models, specifically Hugging Face Embeddings.\n",
        "  - Used for generating text embeddings that can be applied to semantic search, document clustering, and other machine learning tasks.\n",
        "  - Likely involved in enhancing NLP applications by providing robust text processing capabilities.\n",
        "\n",
        "- **langchain**:\n",
        "  - Xuất hiện để hỗ trợ việc tích hợp các mô hình nhúng, cụ thể là Ôm khuôn mặt.\n",
        "  - Được sử dụng để tạo các phần nhúng văn bản có thể áp dụng cho tìm kiếm ngữ nghĩa, phân cụm tài liệu và các tác vụ học máy khác.\n",
        "  - Có khả năng tham gia vào việc nâng cao các ứng dụng NLP bằng cách cung cấp khả năng xử lý văn bản mạnh mẽ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjV9tZxaWZNH"
      },
      "source": [
        "## Code Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uEyUx3sl-6OQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "# transfromers, langchain\n",
        "# einops: tensor, # accelerate: làm việc với CPU, GPU, TPU,\n",
        "# bitsandbytes: Thư viện cung cấp các hàm tính toán hiệu quả cho việc huấn luyện và chạy các mô hình deep learning với các loại dữ liệu ít bit (ví dụ như quantization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB3Pwzg1zeBJ"
      },
      "source": [
        "## 3. LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQLnNaWoMwZ_"
      },
      "source": [
        "### PROMPTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy30rz0LzeBM"
      },
      "source": [
        "- Integrate: Vector Database ở trong LLM\n",
        "\n",
        "1. Cải thiện Prompting:\n",
        "- In-context learning: Zero/One/Few-shot learning.\n",
        "- Chain of Thought: CoT (ex: \"Let's think step by step\")\n",
        "2. Training, Pre-training (start with a randomized weights, continue train) => Quá ngốn tài nguyên.\n",
        "Fine-Tuning (LLMs: Instruction Fine-Tuning: train model on a SPECIAL TASK).\n",
        "\n",
        "(Đối với các task tính toán, có thể sẽ cần sử dụng Instruction Fine-Tuning để cải thiện nhiều).\n",
        "\n",
        "3. Sử dụng LLMs Model khác:\n",
        "  - Dòng Llama: Llama-2-70B thì quá nặng. Llama-2-7b-chat và Llama-3-13b-chat được sử dụng (vì optimize for chat).\n",
        "  Phi-3 rất nhẹ, NHƯNG THỰC TẾ TEST lại thấy kết quả trả về KHÔNG BẰNG Llama3-7B. Chẳng hạn như câu hỏi \"CodeMely là cộng đồng gì?\"\n",
        "\n",
        "  - Mistral-7B: Only 15GB, Mistral-8x22B: about: 59*5 = 300GB.\n",
        "  (Reference: https://youtu.be/eovBbABk3hw?feature=shared )\n",
        "  (có chữ Instruct là model hợp với tinh chỉnh, hướng dẫn)\n",
        "  \"mistralai/Mistral-7B-Instruct-v0.2\": không tối ưu cho chatbot lắm, chẳng hạn tôi cung cấp tên và hỏi lại nó tên tôi là gì.\n",
        "```\n",
        "      system_prompt = \"\"\"\n",
        "\n",
        "      Context information is below.\n",
        "      ---------------------\n",
        "      {context_str}\n",
        "      ---------------------\n",
        "      Given the context information above, I want you to think step by step to answer the query in a crisp manner. In case you don't know the answer, say 'I don't know!'.\n",
        "      Query: {query_str}\n",
        "      Answer:\n",
        "```\n",
        "  - SeaLLM for Tiếng Việt: https://huggingface.co/SeaLLMs/SeaLLM-7B-v2.5 (chú ý: gpt4 và claude vẫn mạnh hơn bạn. model này chỉ mạnh ở phân khúc 7B open-source thôi, dựa trên 1 số benchmark nhất định thôi). link bài: https://www.facebook.com/groups/machinelearningcoban/permalink/1908000919657306/ - 17GB\n",
        "  - Phi-3 (ra sau Mistral-7B tầm 3 hôm):\n",
        "  The repository for microsoft/Phi-3-mini-128k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-128k-instruct.\n",
        "  128k là bản ngữ cảnh, context_window max = 128k, 4k là bản context_window max = 4k.\n",
        "  You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
        "\n",
        "4. Prompt ing for Table data: https://arxiv.org/pdf/2310.09263.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey5Xw-I6MqP7"
      },
      "source": [
        "`query_wrapper_prompt`\n",
        "https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/prompts.py\n",
        "```python\n",
        "\n",
        "__all__ = [\n",
        "    \"Prompt\",\n",
        "    \"PromptTemplate\",\n",
        "    \"SelectorPromptTemplate\",\n",
        "    \"ChatPromptTemplate\",\n",
        "    \"LangchainPromptTemplate\",\n",
        "    \"BasePromptTemplate\",\n",
        "    \"PromptType\",\n",
        "    \"ChatMessage\",\n",
        "    \"MessageRole\",\n",
        "    \"display_prompt_dict\",\n",
        "]\n",
        "```\n",
        "Đọc thêm tại đây: https://github.com/run-llama/llama_index/tree/main/llama-index-core/llama_index/core/prompts\n",
        "\n",
        "- some PromptTemplate you can see here: https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/prompts.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBohsPGx83Gy"
      },
      "source": [
        "## LLM to GGUF format và TỐI ƯU HOÁ MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1FGJKXI81vf"
      },
      "source": [
        "Reference: https://youtu.be/wxQgGK5K0rE?feature=shared\n",
        "- Process of converting any LLM to GGUF format that we'll be able to rent on CPU.\n",
        "- You can do this task even on T4 or even on a CPU machine as well because I'm going to take a small LLM and then do that. (0:02:00)\n",
        "- Now for quantization you can quantize up to two bits. (0:06:00)\n",
        "\n",
        "When working with Google Colab T4, 8-bit and 4-bit quantization techniques are practical for reducing memory usage while maintaining reasonable accuracy. For more aggressive memory reductions, specialized schemes like Q4_K_M, Q2_XS, and Q1_M can be considered, but they come with significant trade-offs in accuracy.\n",
        "\n",
        "- Load llama3-8b ở 8 bit tầm 12GB Ram, llama3-8b ở 4 bit chỉ tầm 9GB Ram.\n",
        "- Quantize 4 bit:\n",
        "```python\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # or load_in_4bit=True for 4-bit quantization\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Optional: use bfloat16 for faster computation with 4-bit\n",
        "    # kiểu dữ liệu sẽ được sử dụng cho các tính toán trong quá trình lượng tử hóa 4-bit. torch.bfloat16 (bfloat16) là một định dạng số học với độ chính xác 16 bit\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Optional: use NormalFloat 4 for 4-bit quantization, chỉ định loại lượng tử hóa 4-bit\n",
        "    bnb_4bit_use_double_quant=True  # Optional: use double quantization for more memory efficiency,\n",
        ")\n",
        "```\n",
        "\n",
        "Em cũng xem qua cái anh bảo, LLM to GGUF format.\n",
        "Llama3-8b lượng tử 4 bit tốc độ cũng đã khá nhanh so với 8 bit, nhanh hơn nhiều so với xài phi3 lượng tử hoá 8 bit.\n",
        "\n",
        "\n",
        "Việc tối ưu hóa mô hình để đạt tốc độ và hiệu suất cao không chỉ dựa vào lượng tử hóa mà còn phụ thuộc vào nhiều yếu tố khác. Dưới đây là một số phương pháp và kỹ thuật bạn có thể sử dụng để cải thiện tốc độ của mô hình.\n",
        "\n",
        "### 1. Lượng Tử Hoá\n",
        "\n",
        "- **Lượng Tử Hoá 4-bit:** Như bạn đã thấy, lượng tử hóa 4-bit mang lại sự cân bằng tốt giữa tốc độ và độ chính xác. Lượng tử hóa xuống 2-bit hoặc 1-bit có thể làm giảm đáng kể chất lượng mô hình, vì vậy bạn nên cẩn thận khi sử dụng các mức lượng tử hóa thấp hơn này.\n",
        "\n",
        "### 2. Sử Dụng Phần Cứng Tối Ưu\n",
        "\n",
        "- **GPU (Graphics Processing Unit):** Sử dụng GPU như NVIDIA T4 có thể tăng tốc độ tính toán đáng kể so với CPU.\n",
        "- **TPU (Tensor Processing Unit):** Nếu bạn có quyền truy cập, TPU cũng có thể là một lựa chọn tốt cho các tác vụ học sâu.\n",
        "\n",
        "### 3. Kỹ Thuật Tối Ưu Hóa Mô Hình\n",
        "\n",
        "- **Distillation (Chưng Cất Mô Hình):** Kỹ thuật này tạo ra một mô hình nhỏ hơn nhưng nhanh hơn bằng cách \"học\" từ một mô hình lớn hơn.\n",
        "- **Pruning (Cắt Tỉa Mô Hình):** Loại bỏ các phần không cần thiết của mô hình để giảm kích thước và tăng tốc độ.\n",
        "\n",
        "### 4. Tối Ưu Hóa Phần Mềm\n",
        "\n",
        "- **ONNX Runtime:** Chuyển đổi mô hình sang định dạng ONNX và sử dụng ONNX Runtime có thể tăng tốc độ suy luận.\n",
        "- **TensorRT:** Sử dụng TensorRT của NVIDIA để tối ưu hóa và tăng tốc các mô hình deep learning trên GPU.\n",
        "\n",
        "### 5. Tối Ưu Hóa Mã Nguồn\n",
        "\n",
        "- **Parallel Processing (Xử Lý Song Song):** Sử dụng các kỹ thuật xử lý song song để tận dụng tối đa tài nguyên phần cứng.\n",
        "- **Efficient Data Loading:** Đảm bảo rằng việc tải dữ liệu vào mô hình không gây ra nghẽn cổ chai.\n",
        "\n",
        "### 6. Sử Dụng Các Thư Viện và Công Cụ Tối Ưu\n",
        "\n",
        "- **Hugging Face Accelerate:** Thư viện này giúp dễ dàng tối ưu hóa mô hình để chạy trên nhiều loại phần cứng khác nhau.\n",
        "- **BitsAndBytes:** Thư viện hỗ trợ lượng tử hóa mô hình với nhiều mức độ khác nhau.\n",
        "\n",
        "### Trải Nghiệm Của Mình\n",
        "\n",
        "- **Lượng tử hóa 4-bit:** Đối với các mô hình mà tôi sử dụng, lượng tử hóa 4-bit đã mang lại sự cân bằng tốt giữa tốc độ và độ chính xác.\n",
        "- **Sử dụng GPU:** Sử dụng GPU mạnh mẽ như NVIDIA A100 hoặc T4 đã giúp cải thiện đáng kể tốc độ suy luận.\n",
        "- **ONNX Runtime:** Chuyển đổi một số mô hình sang ONNX và sử dụng ONNX Runtime để tăng tốc.\n",
        "\n",
        "### Kết Luận\n",
        "\n",
        "Để đạt được tốc độ nhanh và hiệu suất cao như bạn mong muốn, tôi khuyên bạn nên duy trì lượng tử hóa ở mức 4-bit và tận dụng các phần cứng mạnh mẽ như GPU. Đồng thời, sử dụng các kỹ thuật tối ưu hóa phần mềm và mã nguồn sẽ giúp cải thiện tốc độ suy luận của mô hình.\n",
        "\n",
        "Nếu bạn có thêm bất kỳ câu hỏi nào hoặc cần hỗ trợ cụ thể hơn về việc tối ưu hóa mô hình, hãy cho tôi biết!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37dSz6o8WZNI"
      },
      "source": [
        "## Code LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7M89fKg54H",
        "outputId": "7e3ca364-fe0b-4b39-ebd9-8f092f466797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastapi-cli 0.0.3 requires typer>=0.12.3, but you have typer 0.9.4 which is incompatible.\n",
            "gradio 4.31.4 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index # ModuleNotFoundError: No module named 'llama_index'\n",
        "\n",
        "# from llama_index.core.prompts import SimpleInputPrompt\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt, PromptTemplate\n",
        "# llama_index.prompts: Cung cấp các công cụ và cấu trúc để xây dựng và quản lý các prompts, bao gồm:\n",
        "# SimpleInputPrompt cho việc tạo prompts đơn giản.\n",
        "\n",
        "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "## Bao bọc câu truy vấn query_str bởi 1 prompt đơn giản.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjY9n8Mcqdj0",
        "outputId": "a980e534-5e81-4f8e-caec-9d66ff739d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has been saved.\n"
          ]
        }
      ],
      "source": [
        "# # Cách login 1:\n",
        "# # huggingface login: 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\n",
        "# !huggingface-cli login\n",
        "\n",
        "# # Cách login 2:\n",
        "\n",
        "# Lưu token vào biến môi trường\n",
        "# !setx HUGGINGFACE_TOKEN \"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\"  # window\n",
        "import os\n",
        "os.environ['HUGGINGFACE_TOKEN'] = \"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\"\n",
        "\n",
        "import os\n",
        "from huggingface_hub import HfFolder\n",
        "\n",
        "# Lấy token từ biến môi trường\n",
        "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "\n",
        "# Lưu token vào cấu hình huggingface\n",
        "HfFolder.save_token(token)  # Lưu token để sử dụng cho các hoạt động API\n",
        "\n",
        "print(\"Token has been saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NteBhpkoWZNJ"
      },
      "outputs": [],
      "source": [
        "# # Cách 1: Sử dụng Transformers và HuggingFaceLLM\n",
        "\n",
        "# # Load model and tokenizer directly, by passing:\n",
        "# # The repository for microsoft/Phi-3-mini-128k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-128k-instruct.\n",
        "# # You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
        "# # https://huggingface.co/microsoft/Phi-3-mini-128k-instruct?library=true (mỗi model có cách load transformers riêng)\n",
        "\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\n",
        "\n",
        "# # Sau đó, bạn có thể khởi tạo HuggingFaceLLM với các đối tượng đã tải\n",
        "# llm = HuggingFaceLLM(\n",
        "#     context_window=4096,\n",
        "#     max_new_tokens=256,\n",
        "#     tokenizer=tokenizer,\n",
        "#     model=model,\n",
        "#     system_prompt=system_prompt,\n",
        "#     query_wrapper_prompt=query_wrapper_prompt,\n",
        "#     generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "#     device_map=\"auto\",\n",
        "#     model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True}\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "937229e595b1435e8a5aff2d47cf4375",
            "d05fc79ca755409ca1978b285b03af4e",
            "6345eaaf99d74c4d8192200f3d50a368",
            "70d4887bf47c492a927db978ac3a3573",
            "b13df1cbcaca4490ac98fac7f7bd8eda",
            "a938aeb230374e9fa8dc036e2f19b27f",
            "2543dc244515455197467a7592cba219",
            "9f5cef969e314d3b840b978ccb6836f3",
            "e2d499d208434ccab96a4deaade74383",
            "75738d7ac73c4634af8c140cdfc6b1a0",
            "f7295d18bf134337b6617c567f280993",
            "f626731a93ab4a698ab3cf2e5a05828d",
            "158f92afc2c346ecb298854ad3e20e02",
            "687e18bdb8a344eeacdd9a80a8f3aed6",
            "3c19a78d35544746b4f3d3f792ecbb54",
            "d4395c86ebaa4ae692825013f719ef68",
            "97b303d8f2134494a67e032762088aa3",
            "3de89384225647f6a9a0d1eba31bd5b5",
            "03a8c0f7546a4c248f646ccbf4460df9",
            "ca0237ce394e42eab4c0fb963ae50792",
            "7a3ef7267abf4a73a0d2dd2c952c673a",
            "5ade93d1a14c4351a4838430052c7689",
            "f2ae0e10eafe43e38c303a3337fe7e53",
            "e906734861ea4ce5bde63c816fc164ef",
            "2d47bf7016484679b34d54fec6738013",
            "724181b5f2e74a2493e0f0486c0b8d6e",
            "00cb1b052d5943b0893297f884ef1311",
            "75d7e73257854fb390bbb10154c35074",
            "c289f6970d444644902daa850e8d62cd",
            "39bd24d84b1440409830eab68c199f85",
            "d12cd9f885074e9b9ad93489f01859e5",
            "937f2140f1be41e79972a3c2a1267cda",
            "caf82a9d37b44367bc91a54bd86c369e",
            "675074d5a61e46608a772c51fa2ee8d1",
            "108c85db35db4b4eb40fee4b731c5a1e",
            "bcb93d98abb44c208ecaa73fa96ff957",
            "3fe0c7bbeda34b2a8c03d0000fe8695c",
            "ea6791d5445946e683555ff90db0512d",
            "dd0de5a8ef6c417ab2fce321f21a0028",
            "72152e757a604f30ad54823aa61c8e79",
            "204dd54259b34fa78630d9cfef4a2e7b",
            "55dc696e88584a8dbd68b8b67ec6065b",
            "21fabc2aaabd4a11b7cd2e72e3b7d273",
            "7d72db825eba44d3bcfaf7df0bab0454",
            "3eb406b724104ef49e74ebd7c5c1a29f",
            "679bc0dca08940b69e2d6db3b72dbb3e",
            "9680b6df6ca64a07b74b52f2475c9a29",
            "d4dea10e00e04805aa769f915d820bba",
            "ed335d1ac3454d3baa4ff12e9f4e44a7",
            "8ec8e6ea19d74ee2af17e978ed52680e",
            "62893941b58c44178816aa751abe2b03",
            "6839f894ad094a9ebd6a52e80ad219b0",
            "003b2c20c53547f2b64cba373f5ba382",
            "4035e681343849068e482ec362c72271",
            "5d70947f76c74722875aafd8a50f2ab6"
          ]
        },
        "id": "uO1WaqeXfzah",
        "outputId": "98fbecbe-f18f-4b17-f7c1-f7bf880bd6c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.10.37.post1)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.11.0)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.30.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (9.4.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>2 in /usr/local/lib/python3.10/dist-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.30.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.11)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "937229e595b1435e8a5aff2d47cf4375",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f626731a93ab4a698ab3cf2e5a05828d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2ae0e10eafe43e38c303a3337fe7e53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "675074d5a61e46608a772c51fa2ee8d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eb406b724104ef49e74ebd7c5c1a29f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cách 2:  Sử dụng llama-index-llms-huggingface\n",
        "\n",
        "# Cài đặt gói llama-index-llms-huggingface, cung cấp các công cụ để làm việc với mô hình ngôn ngữ từ Hugging Face.\n",
        "%pip install llama-index-llms-huggingface\n",
        "# https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/llm/huggingface.ipynb\n",
        "\n",
        "# Import lớp HuggingFaceLLM từ thư viện llama_index để làm việc với mô hình ngôn ngữ.\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "name_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# name_model=\"meta-llama/Llama-2-13b-chat-hf\", # OUT OF MEMORY\n",
        "# name_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "name_model = \"SeaLLMs/SeaLLM-7B-v2.5\"  # 16, 17GB OUT OF MEMORY\n",
        "# name_model = \"meta-llama/Meta-Llama-3-8B-Instruct\" # Cẩn thận 1 số lỗi lặp khi responds.\n",
        "# name_model = \"microsoft/Phi-3-mini-128k-instruct\" # ngữ cảnh 128k. ĐÃ TEST KHA KHÁ RAG with CodeMely, thấy Phi3 thua llama-8b nhiều\n",
        "# name_model = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "\n",
        "# The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
        "# Define quantization configuration\n",
        "# Define quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # or load_in_4bit=True for 4-bit quantization\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Optional: use bfloat16 for faster computation with 4-bit\n",
        "    # kiểu dữ liệu sẽ được sử dụng cho các tính toán trong quá trình lượng tử hóa 4-bit. torch.bfloat16 (bfloat16) là một định dạng số học với độ chính xác 16 bit\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Optional: use NormalFloat 4 for 4-bit quantization, chỉ định loại lượng tử hóa 4-bit\n",
        "    bnb_4bit_use_double_quant=True  # Optional: use double quantization for more memory efficiency,\n",
        ")\n",
        "\n",
        "\n",
        "# Khởi tạo đối tượng HuggingFaceLLM với các thông số cụ thể.\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window= 2048, # 4096, # giảm thử xem tốc độ có tăng ko  # cửa sổ ngữ cảnh 4096 tokens.\n",
        "    max_new_tokens= 256,  # lượng token tối đa có thể sinh 256.\n",
        "\n",
        "    # 4 tham số ở trên:\n",
        "    tokenizer_name = name_model,  # tách query và mã hóa.\n",
        "    model_name = name_model,\n",
        "    system_prompt=system_prompt, # We can update system_prompt at under\n",
        "    query_wrapper_prompt=query_wrapper_prompt,  # Đặt prompt bao quanh truy vấn để định dạng câu hỏi.\n",
        "\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},  # Thiết lập các thông số sinh văn bản.\n",
        "    # temperature điều chỉnh độ ngẫu nhiên của các dự đoán văn bản (0.0 == chắc chắn)\n",
        "    # do_sample: True => mô hình sẽ chọn ngẫu nhiên các từ dựa trên phân phối xác suất của chúng. False => chọn từ có xác suất cao nhất, mà ko có sự ngẫu nhiên giống temperature=0.0\n",
        "    device_map=\"auto\",  # Tự động sử dụng thiết bị tối ưu cho việc tính toán (CPU/GPU). device_map=\"cpu\"\n",
        "\n",
        "      model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,  # chỉ định kiểu dữ liệu sẽ được sử dụng cho các tính toán trong PyTorch. torch.float16 (hay còn gọi là half-precision) là một định dạng số học với độ chính xác 16 bit,\n",
        "        \"quantization_config\": quantization_config  # Sử dụng cấu hình quantization\n",
        "    }\n",
        "    # model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_4bit\":True},  # Sử dụng quantization để giảm dung lượng mô hình.\n",
        "    # The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
        "    # torch_dtype: kiểu dữ liệu mô hình sử dụng trong PyTorch. torch.float16 (hay còn gọi là half-precision) là một kiểu dữ liệu có độ chính xác thấp hơn so với kiểu dữ liệu mặc định (float32), nhưng nó giảm dung lượng bộ nhớ cần thiết và có thể tăng tốc độ tính toán trên các thiết bị hỗ trợ.\n",
        "    # load_in_8bit, load_in_4_bit: Tham số này chỉ định rằng mô hình nên được tải với dữ liệu được nén xuống 8 bit, mặc dù có thể ảnh hưởng đến độ chính xác của mô hình.\n",
        "    # Mặc định: Default (float32)32-bit floating point precision (float32) for their weights and computations\n",
        "    # Có thể thử 1 số loại QUANTIZTE ĐẶC BIỆT HƠN: Q4, Q3, Q2, Q1.\n",
        "    )\n",
        "# không chuyển T4 sẽ bị dính: ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saw4JQHRrtou"
      },
      "outputs": [],
      "source": [
        "# WE CAN UPDATE this parameter under that\n",
        "# Function to print llm model parameters\n",
        "def print_llm_params(llm):\n",
        "    print(\"Model parameters:\")\n",
        "    print(f\"Context window: {llm.context_window}\")\n",
        "    print(f\"Max new tokens: {llm.max_new_tokens}\")\n",
        "    print(f\"Tokenizer name: {llm.tokenizer_name}\")\n",
        "    print(f\"Model name: {llm.model_name}\")\n",
        "    print(f\"System prompt: {llm.system_prompt}\")\n",
        "    print(f\"Generate kwargs: {llm.generate_kwargs}\")\n",
        "    print(f\"Device map: {llm.device_map}\")\n",
        "    print(f\"Model kwargs: {llm.model_kwargs}\")\n",
        "\n",
        "# Print model parameters\n",
        "print_llm_params(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9wMBhf1tCmM"
      },
      "outputs": [],
      "source": [
        "# Cách 3: chỉ có infer model từ huggingface mà lại phải xài đến cả transformer với pytorch như vậy hơi ngốn nguồn lực\n",
        "!pip install llamacpp-python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRnhB9WkWZNJ"
      },
      "source": [
        "Sự khác biệt chính giữa hai cách tải mô hình LLMs Phi-3 mà bạn đã đề cập đến, như trong \"Code 1\" và \"Code 2\", có thể ảnh hưởng đến hiệu suất và sử dụng tài nguyên hệ thống một cách đáng kể. Dưới đây là một số điểm khác biệt cơ bản giữa hai cách tiếp cận này:\n",
        "\n",
        "- Code 1: Sử dụng Transformers và HuggingFaceLLM\n",
        "\n",
        "    - **Cách tiếp cận**: Trong cách này, bạn sử dụng `AutoTokenizer` và `AutoModelForCausalLM` từ thư viện `transformers` để tải trực tiếp tokenizer và mô hình. Điều này đảm bảo rằng bạn đang sử dụng các lớp được tối ưu hóa cho kiểu mô hình cụ thể.\n",
        "    - **Tùy chỉnh và linh hoạt**: Bạn có khả năng tùy chỉnh cao hơn trong việc xử lý các đối tượng tokenizer và mô hình, cũng như các tham số liên quan đến sinh token và điều khiển quá trình tạo văn bản.\n",
        "    - **Sử dụng tài nguyên**: Cách tiếp cận này có thể tiêu tốn nhiều bộ nhớ hơn do cách thức mô hình được tải và xử lý trong bộ nhớ, nhất là trên các thiết bị có hạn chế về RAM như Google Colab.\n",
        "\n",
        "- Code 2: Sử dụng llama-index-llms-huggingface\n",
        "\n",
        "    - **Cách tiếp cận**: Dùng thư viện `llama-index-llms-huggingface` để tải và xử lý mô hình. Thư viện này có thể có những cài đặt và tối ưu hoá đặc biệt để làm việc với các mô hình ngôn ngữ lớn một cách hiệu quả hơn.\n",
        "    - **Tích hợp**: Thư viện có thể đã tích hợp các kỹ thuật như nén mô hình, quantization, hoặc sử dụng định dạng dữ liệu ít tốn kém tài nguyên hơn (ví dụ: `torch.float16` và `load_in_8bit`). Điều này giúp giảm đáng kể lượng RAM cần thiết để tải và chạy mô hình.\n",
        "    - **Hiệu năng và ổn định**: Do cách tối ưu hóa và quản lý tài nguyên tốt hơn, cách này có thể cho phép bạn chạy mô hình một cách ổn định hơn trên các nền tảng có hạn chế tài nguyên như Google Colab.\n",
        "\n",
        "\n",
        "Bug:\n",
        "```\n",
        "# Initialize the Hugging Face language model with specific parameters for operation.\n",
        "# ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate\n",
        "# use: !pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "\n",
        "```\n",
        "=> Phải chuyển qua dùng GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peQ9WyWQWZNJ"
      },
      "source": [
        "## 1. Embedding Model\n",
        "\n",
        "**Embedding Model** là các mô hình học máy chuyên để biến đổi dữ liệu văn bản thành dạng vector đặc trưng (embeddings) trong không gian số. Các vector này mang thông tin ngữ nghĩa, giúp máy tính có thể \"hiểu\" và xử lý văn bản hiệu quả hơn.\n",
        "\n",
        "Trong bài toán Retrieval-Augmented Generation (RAG), embeddings giúp cải thiện việc tìm kiếm và truy xuất thông tin, bằng cách xác định mối quan hệ ngữ nghĩa giữa câu hỏi đưa ra và cơ sở dữ liệu kiến thức, từ đó chọn ra thông tin liên quan nhất để tạo ra câu trả lời.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl01Q2MIWZNK"
      },
      "source": [
        "### Research Embedding Model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPc0iUiLWZNK"
      },
      "source": [
        "\n",
        "Xếp theo kích thước Model:\n",
        "\n",
        "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "   - 22.7 triệu tham số, kích thước model nhẹ (90.9 MB)\n",
        "- https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2\n",
        "- https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
        "   - 109 triệu tham số, kích thước lớn hơn (438 MB).\n",
        "- https://huggingface.co/BAAI/bge-large-en-v1.5\n",
        "   - 1.34 tỷ tham số, model rất lớn (1.34 GB)\n",
        "- https://huggingface.co/intfloat/multilingual-e5-large/\n",
        "\n",
        "Đúc kết:\n",
        "   - **Nếu cần tốc độ**: **all-MiniLM-L6-v2** hoặc **all-MiniLM-L12-v2** là sự lựa chọn tốt vì chúng nhẹ và nhanh.\n",
        "   - **Nếu cần độ chính xác và sâu sắc ngữ nghĩa cao**: **all-mpnet-base-v2** hoặc **BAAI/bge-large-en-v1.5** là phù hợp hơn.\n",
        "   - **Nếu làm việc với đa ngôn ngữ**: **intfloat/multilingual-e5-large** sẽ là lựa chọn tốt nhất.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz7DebeTWZNK"
      },
      "source": [
        "\n",
        "#### Giải thích Embedding Model và Ứng dụng trong Bài Toán RAG\n",
        "\n",
        "**Embedding Model** là các mô hình học máy chuyên để biến đổi dữ liệu văn bản thành dạng vector đặc trưng (embeddings) trong không gian số. Các vector này mang thông tin ngữ nghĩa, giúp máy tính có thể \"hiểu\" và xử lý văn bản hiệu quả hơn. Trong bài toán Retrieval-Augmented Generation (RAG), embeddings giúp cải thiện việc tìm kiếm và truy xuất thông tin, bằng cách xác định mối quan hệ ngữ nghĩa giữa câu hỏi đưa ra và cơ sở dữ liệu kiến thức, từ đó chọn ra thông tin liên quan nhất để tạo ra câu trả lời.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yAmsb5LWZNK"
      },
      "source": [
        "\n",
        "\n",
        "#### Đúc Kết Chi Tiết về 5 Mô Hình Embedding\n",
        "\n",
        "1. **all-MiniLM-L6-v2**\n",
        "   - **Kích thước và Tham số**: Kích thước nhẹ với 22.7 triệu tham số.\n",
        "   - **Đặc Điểm, Ứng Dụng**: Mô hình này sử dụng không gian vector 384 chiều, tối ưu cho tốc độ và hiệu suất trong môi trường có tài nguyên hạn chế, phù hợp cho các tác vụ như tìm kiếm ngữ nghĩa, phân cụm và so sánh câu.\n",
        "   - **[Link Mô Hình](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)**\n",
        "\n",
        "2. **all-MiniLM-L12-v2**\n",
        "   - **Kích thước và Tham số**: Kích thước nhẹ với 33.4 triệu tham số.\n",
        "   - **Đặc Điểm, Ứng Dụng**: Biến thể của MiniLM với lớp mạng sâu hơn, cho phép hiểu biết ngữ nghĩa sâu hơn, thích hợp cho các ứng dụng yêu cầu độ chính xác cao hơn trong phân tích ngữ nghĩa.\n",
        "   - **[Link Mô Hình](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)**\n",
        "\n",
        "3. **all-mpnet-base-v2**\n",
        "   - **Kích thước và Tham số**: 109 triệu tham số, kích thước lớn (438 MB).\n",
        "   - **Đặc Điểm, Ứng Dụng**: Mô hình biểu diễn câu vào không gian 768 chiều, thích hợp cho các tác vụ đòi hỏi sự hiểu biết ngữ nghĩa sâu sắc như phân tích cảm xúc tinh vi hoặc phân loại văn bản.\n",
        "   - **[Link Mô Hình](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)**\n",
        "\n",
        "4. **BAAI/bge-large-en-v1.5**\n",
        "   - **Kích thước và Tham số**: 1.34 tỷ tham số, model rất lớn (1.34 GB).\n",
        "   - **Đặc Điểm, Ứng Dụng**: Mô hình này nhằm mục đích cải thiện phân phối độ tương đồng, tối ưu cho các tác vụ tìm kiếm và phân loại trên quy mô lớn, đặc biệt là trong môi trường tiếng Anh.\n",
        "   - **[Link Mô Hình](https://huggingface.co/BAAI/bge-large-en-v1.5)**\n",
        "\n",
        "5. **intfloat/multilingual-e5-large**\n",
        "   - **Kích thước và Tham số**: 560 triệu tham số, model rất lớn (2.24 GB).\n",
        "   - **Đặc Điểm, Ứng Dụng**: Hỗ trợ 94 ngôn ngữ, mô hình này được tối ưu cho các ứng dụng đa ngôn ngữ trên quy mô lớn như tìm kiếm đa ngôn ngữ và phân loại ngôn ngữ chéo.\n",
        "   - **[Link Mô Hình](https://huggingface.co/intfloat/multilingual-e5-large)**\n",
        "   - ***Với việc truy vấn Tiếng Việt model song ngữ này có lợi hơn.***\n",
        "   \n",
        "Mỗi mô hình này mang những đặc điểm và ứng dụng riêng, phù hợp với nhu cầu và mục đích sử dụng khác nhau trong lĩnh vực xử lý ngôn ngữ tự nhiên, từ các tác vụ đơn giản đến các tác vụ phức tạp đa ngôn ngữ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K-zRZ1GWZNK"
      },
      "source": [
        "## Code Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dmdme0UKWZNK"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index # Cài đặt thư viện llama_index để làm việc với cơ sở dữ liệu vector.\n",
        "!pip install llama-index-embeddings-langchain # Cài đặt một phần của thư viện llama_index để hỗ trợ nhúng văn bản dùng langchain.\n",
        "!pip install langchain # Cài đặt thư viện langchain, giúp tích hợp mô hình ngôn ngữ từ Hugging Face.\n",
        "!pip install sentence_transformers # Cài đặt thư viện sentence_transformers để nhúng văn bản.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po0G_k2NWZNK"
      },
      "outputs": [],
      "source": [
        "# Cách 1: (dự đoán RAM tăng như lúc load model LLMs bằng cách này)\n",
        "\n",
        "# # from openagent import HuggingFaceEmbedding\n",
        "# # embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
        "\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# # Load tokenizer and model\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
        "# model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
        "\n",
        "# # Encode some text\n",
        "# inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
        "# outputs = model(**inputs)\n",
        "\n",
        "# # Get embeddings\n",
        "# embeddings = outputs.last_hidden_state\n",
        "# print(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTrDXNyVPJyC"
      },
      "outputs": [],
      "source": [
        "# # Cách 2\n",
        "# # Import thư viện langchain để nhúng văn bản.\n",
        "# !pip install langchain # Cài đặt thư viện langchain, giúp tích hợp mô hình ngôn ngữ từ Hugging Face.\n",
        "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# # sentence_transformers cho các mô hình nhúng, embedding model\n",
        "# !pip install sentence_transformers # Cài đặt thư viện sentence_transformers để nhúng văn bản.\n",
        "\n",
        "# embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WUJSLFYbyzc"
      },
      "outputs": [],
      "source": [
        "# # # Cách 3: kết hợp Langchain với HuggingFace\n",
        "!pip install -U langchain-community\n",
        "# Import thư viện langchain để nhúng văn bản.\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Import thư viện langchain từ llama_index.\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "# # Khởi tạo mô hình nhúng văn bản sử dụng mô hình 'all-mpnet-base-v2'.\n",
        "# embed_model=LangchainEmbedding(\n",
        "#     HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")) # 438MB\n",
        "\n",
        "\n",
        "# Create and dl embeddings instance\n",
        "# embed_model=LangchainEmbedding(\n",
        "#     HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\") # 1.34 GB, model English không khai thác được tối đa khi query đầu vào ở dạng vietnamese\n",
        "# )\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large-instruct\") # PHÙ HỢP CHO MULTILINGUAL\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSJPhq3DfFyM"
      },
      "source": [
        "Các cách load mô hình embedding này khác nhau về thư viện được sử dụng, phương pháp tích hợp, và mức độ trừu tượng của mã nguồn:\n",
        "\n",
        "### Cách 1: Sử dụng trực tiếp Transformers từ Hugging Face\n",
        "\n",
        "- Đây là cách trực tiếp nhất, sử dụng thư viện `transformers` của Hugging Face để tải tokenizer và mô hình.\n",
        "- Cho phép bạn có toàn quyền kiểm soát quá trình mã hoá và xử lý dữ liệu đầu vào.\n",
        "- Bạn cần tự quản lý các tensors và làm việc trực tiếp với PyTorch.\n",
        "- Đòi hỏi hiểu biết tốt về cách mô hình hoạt động và cách thức xử lý dữ liệu trong PyTorch.\n",
        "\n",
        "### Cách 2: Sử dụng thư viện Langchain\n",
        "\n",
        "- Langchain là một thư viện cao cấp hơn, giúp tích hợp mô hình ngôn ngữ từ Hugging Face và có khả năng nhúng văn bản dễ dàng hơn.\n",
        "- Thư viện này cung cấp một API trừu tượng hơn so với cách sử dụng trực tiếp `transformers`, có thể giảm bớt độ phức tạp khi làm việc với mô hình.\n",
        "- Langchain có thể đóng gói các bước cần thiết vào các phương thức cao cấp, giúp bạn không cần phải quản lý tensor hoặc chi tiết cụ thể của PyTorch.\n",
        "\n",
        "### Cách 3: Kết hợp Langchain với Llama-index\n",
        "\n",
        "- Llama-index là một thư viện chuyên biệt cho việc tạo chỉ mục và truy vấn dữ liệu dựa trên ngữ nghĩa, và thường được sử dụng trong các hệ thống thông tin và truy vấn ngữ nghĩa.\n",
        "- Việc kết hợp `LangchainEmbedding` từ `llama-index` với `HuggingFaceEmbeddings` cho phép bạn tận dụng cả hai thế mạnh: khả năng nhúng ngữ nghĩa từ langchain và hệ thống chỉ mục mạnh mẽ từ llama-index.\n",
        "- Điều này cho phép tích hợp sâu vào hệ thống có sẵn của llama-index, đồng thời giữ được sự đơn giản trong quá trình triển khai do langchain cung cấp.\n",
        "\n",
        "Mỗi cách đều có những ưu và nhược điểm riêng, tùy thuộc vào mục tiêu cụ thể và trình độ kỹ thuật của người phát triển:\n",
        "\n",
        "- Nếu bạn cần kiểm soát chặt chẽ quá trình mã hoá và muốn hiểu sâu về cách thức hoạt động của mô hình, cách 1 có thể phù hợp hơn.\n",
        "- Nếu bạn muốn đơn giản hoá quá trình và cần một giải pháp nhanh chóng, cách 2 là lựa chọn tốt.\n",
        "- Nếu bạn muốn tích hợp nhúng vector vào một hệ thống chỉ mục có sẵn và tận dụng khả năng truy vấn ngữ nghĩa, cách 3 sẽ hữu ích.\n",
        "\n",
        "Cách 2 và cách 3 đều liên quan đến việc sử dụng mô hình nhúng từ Hugging Face, nhưng chúng khác biệt về cách thức tích hợp và mục đích sử dụng:\n",
        "\n",
        "### Cách 2: Sử dụng Langchain\n",
        "\n",
        "- Thư viện Langchain cung cấp các wrappers và utilities cho việc tích hợp các mô hình nhúng ngôn ngữ của Hugging Face, giúp quá trình này trở nên dễ dàng và trực quan hơn.\n",
        "- Với Langchain, bạn không cần quan tâm nhiều đến các chi tiết kỹ thuật của việc xử lý tensor hoặc quản lý mô hình trong PyTorch, vì nó đảm nhận phần lớn công việc đó.\n",
        "- Nó chủ yếu hướng đến việc đơn giản hoá việc nhúng ngôn ngữ và không nhất thiết tích hợp sâu vào các hệ thống chỉ mục.\n",
        "\n",
        "### Cách 3: Kết hợp Langchain với Llama-index\n",
        "\n",
        "- Khi kết hợp LangchainEmbedding từ Llama-index với HuggingFaceEmbeddings, bạn có cơ hội tận dụng được khả năng nhúng ngữ nghĩa từ Langchain cùng với cơ chế chỉ mục của Llama-index.\n",
        "- Llama-index không chỉ cung cấp khả năng nhúng ngữ nghĩa mà còn hỗ trợ việc tạo chỉ mục và truy vấn dữ liệu dựa trên nội dung ngữ nghĩa của các nhúng đó. Điều này rất hữu ích cho việc xây dựng hệ thống truy vấn thông minh, nơi bạn có thể tìm kiếm thông tin dựa trên ý nghĩa chứ không chỉ dựa trên từ khóa.\n",
        "- \"Tận dụng tốt hơn khả năng truy vấn\" ở đây có nghĩa là bạn không chỉ tạo ra các vectơ nhúng mà còn có thể nhanh chóng tìm kiếm và truy xuất thông tin có liên quan trong một lượng lớn dữ liệu, nhờ vào cơ sở dữ liệu chỉ mục mà Llama-index cung cấp.\n",
        "\n",
        "Tóm lại, cách 3 nâng cao khả năng của cách 2 bằng cách tích hợp không chỉ việc nhúng ngôn ngữ mà còn việc quản lý và truy vấn dữ liệu dựa trên những nhúng này. Điều này rất hữu ích cho các ứng dụng cần tìm kiếm và truy xuất thông tin dựa trên nội dung ngữ nghĩa như các hệ thống hỗ trợ quyết định, trợ lý ảo, hoặc công cụ tìm kiếm thông minh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkemBJowWZNL"
      },
      "outputs": [],
      "source": [
        "# Import thư viện langchain từ llama_index.\n",
        "!pip install llama-index-embeddings-langchain # Cài đặt một phần của thư viện llama_index để hỗ trợ nhúng văn bản dùng langchain.\n",
        "\n",
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "# from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "# llama_index.core Chứa các lớp cơ bản và chức năng của llama_index\n",
        "# link: # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
        "# VectorStoreIndex dùng để tạo chỉ mục (index) cho mục đích truy vấn các vector.\n",
        "# SimpleDirectoryReader giúp đọc dữ liệu từ thư mục chỉ định.\n",
        "# ServiceContext quản lý thông tin ngữ cảnh của dịch vụ, bao gồm các cài đặt cho việc chia nhỏ dữ liệu và liên kết với mô hình ngôn ngữ và mô hình nhúng.\n",
        "\n",
        "# Tạo một ngữ cảnh dịch vụ mặc định với chunk_size là 1024, điều này có nghĩa là dữ liệu sẽ được chia nhỏ\n",
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size= 512, # 1024 # giảm thử xem tốc độ,  # Kích thước của mỗi khối dữ liệu khi xử lý.\n",
        "    llm=llm,          # Mô hình ngôn ngữ lớn để xử lý và sinh văn bản.\n",
        "    embed_model=embed_model  # Mô hình nhúng để chuyển đổi văn bản thành vector.\n",
        ")\n",
        "\n",
        "# In ra thông tin của service_context để kiểm tra các cài đặt hiện tại.\n",
        "print(service_context)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AqH5D5cWZNL"
      },
      "source": [
        "## 2. Vector Database(VectorStore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoaP2Qa0WZNL"
      },
      "source": [
        "https://cloud.llamaindex.ai/api-key:\n",
        "`llx-TyfBFekZjS4u9HGA9Mcs7Jt2J4efPmcnjLYJfWwIQDnomUGX`\n",
        "\n",
        "Reference: Parse file for optimal RAG: https://github.com/run-llama/llama_parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaDlhOs0Zw3Q"
      },
      "outputs": [],
      "source": [
        "# Sử dụng SimpleDirectoryReader từ thư viện llama_index để đọc các tài liệu từ thư mục được chỉ định.\n",
        "# Thư mục này chứa các tài liệu mà bạn muốn tạo chỉ mục và truy vấn.\n",
        "documents=SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "# # Kiểm tra các tài liệu đã đọc được bằng cách in ra biến 'documents'.\n",
        "# print(documents)\n",
        "\n",
        "# Sử dụng VectorStoreIndex từ llama_index để tạo chỉ mục vector từ các tài liệu đã SimpleDirectoryReader.\n",
        "# Chỉ mục này cho phép tìm kiếm và truy xuất thông tin dựa trên nội dung của các tài liệu.\n",
        "index=VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Chuyển đổi chỉ mục vector thành một công cụ truy vấn (query engine) mà bạn có thể sử dụng để thực hiện truy vấn.\n",
        "# Công cụ này sẽ trả lời các câu hỏi bằng cách tìm kiếm thông tin liên quan trong cơ sở dữ liệu của bạn.\n",
        "query_engine=index.as_query_engine()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVW-uWdmrnp6"
      },
      "outputs": [],
      "source": [
        "# TẠO: query_engine_llamaparser\n",
        "import nest_asyncio\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# Thay thế \"llx-...\" bằng API key của bạn từ LlamaIndex\n",
        "parser = LlamaParse(\n",
        "    api_key= \"llx-TyfBFekZjS4u9HGA9Mcs7Jt2J4efPmcnjLYJfWwIQDnomUGX\", # \"llx-YourAPIKeyHere\" ,\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Đọc tài liệu từ thư mục và phân tích với LlamaParse\n",
        "file_extractor = {\".pdf\": parser}\n",
        "\n",
        "# THÊM file_extractor\n",
        "documents = SimpleDirectoryReader(\"/content/data\", file_extractor=file_extractor).load_data()\n",
        "\n",
        "# Tạo chỉ mục vector từ tài liệu\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Tạo công cụ truy vấn từ chỉ mục\n",
        "query_engine_llamaparser = index.as_query_engine()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ2VB8GedswJ"
      },
      "outputs": [],
      "source": [
        "# So sánh 2 con query_engine\n",
        "\n",
        "# Example query\n",
        "input_query = \"CodeMely có những hoạt động gì (trả lời bằng tiếng việt). Đưa ra lời khuyên về hướng phát triển cộng đồng\"\n",
        "\n",
        "# Query using both engines and compare response times\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "response = query_engine.query(input_query)\n",
        "print(f\"Query Engine Response: {response}\")\n",
        "print(f\"Time taken: {time.time() - start_time} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "response_llamaparser = query_engine_llamaparser.query(input_query)\n",
        "print(f\"Query Engine LlamaParse Response: {response_llamaparser}\")\n",
        "print(f\"Time taken: {time.time() - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYfDyeBtWZNL"
      },
      "source": [
        "`SimpleDirectoryReader` là một component của thư viện llama-index, mà có chức năng đọc và chuyển đổi tài liệu từ một thư mục cụ thể vào trong chương trình.\n",
        "1. **Đọc Tài liệu**: Nó sẽ duyệt qua tất cả các tệp trong thư mục chỉ định và đọc nội dung của chúng. Thông thường, nó có thể xử lý các định dạng tài liệu phổ biến như .txt, .pdf, .docx, v.v.\n",
        "\n",
        "2. **Tiền xử lý**: Trong quá trình đọc tài liệu, `SimpleDirectoryReader` có thể thực hiện một số bước tiền xử lý cơ bản như loại bỏ các ký tự không cần thiết, chuyển đổi nội dung tài liệu thành dạng chuỗi văn bản thuần túy (plain text), và tách từ (tokenization) nếu cần.\n",
        "\n",
        "3. **Chuyển đổi dữ liệu**: Sau khi nội dung được đọc và tiền xử lý, nó sẽ chuyển đổi các tài liệu thành một định dạng mà llama-index có thể sử dụng để tạo chỉ mục vectơ hoặc để trực tiếp sử dụng trong các hoạt động như truy vấn hoặc huấn luyện mô hình.\n",
        "\n",
        "4. **Lưu Trữ Tạm thời**: `SimpleDirectoryReader` có thể lưu trữ nội dung tài liệu trong một cấu trúc dữ liệu tạm thời, chẳng hạn như một danh sách hoặc một dictionary trong Python, trước khi chúng được truyền đến các bước tiếp theo như chỉ mục hoá hoặc nhúng vectơ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWE_cG-isX1C"
      },
      "outputs": [],
      "source": [
        "# # Function to update system prompt and perform query\n",
        "# def update_prompt_and_query(system_prompt, input_query):\n",
        "#     \"HÀM CHO PHÉP TINH CHỈNH PROMP FOR PROMPTING ENGINEERING\"\n",
        "#     llm.system_prompt = system_prompt # update system_prompt for llm\n",
        "#     print(f\"Updated `System prompt`: {llm.system_prompt}\")\n",
        "#     response = query_engine.query(input_query)\n",
        "#     return str(response)\n",
        "\n",
        "# # Example usage\n",
        "# system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "# # system_prompt = \"\" # CodeMely có các hoạt động sau: Gặp gỡ chuyên gia, Có cơ hội giao lưu, học hỏi kinh nghiệm từ các chuyên gia trong ngành IT, Trải nghiệm thực tế, Tham gia vào các dự án thực tế, hoặc thực hiện các bài tập lập trình để đánh giá năng lực, Hội thảo và chia sẻ, Tổ chức các cuộc thi lập trình chuyên nghiệp, MeLy Show, Các hoạt động khác như talkshow, tham quan công ty, hội thảo chuyên đề, và nhiều hoạt động giao lưu trực tuyến và ngoại tuyến. <class 'str'>\n",
        "# input_query = \"CodeMely có những hoạt động gì (trả lời bằng tiếng việt)\" # CodeMely có các hoạt động sau: tổ chức các cuộc thi lập trình chuyên nghiệp như Codeforces Round #812 và #842, MeLy Show là chuỗi sự kiện kết nối với các lập trình viên, talkshow, tham quan công ty, hội thảo chuyên đề, v ...\n",
        "\n",
        "# response = update_prompt_and_query(system_prompt, input_query)\n",
        "# print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0zGajGWZNL"
      },
      "outputs": [],
      "source": [
        "response=query_engine.query(\"Hello. Can you help me?\")\n",
        "print(response)\n",
        "\n",
        "def predict(input, history):\n",
        "  response = query_engine.query(input)\n",
        "  return str(response)\n",
        "\n",
        "def predict_llamaparser(input, history):\n",
        "  response = query_engine_llamaparser.query(input)\n",
        "  return str(response)\n",
        "\n",
        "\n",
        "# Test the query function\n",
        "history = []\n",
        "query = \"Thời tiết hôm nay ở Hà Nội thế nào?\"\n",
        "response = predict(query, history)\n",
        "print(response, type(response))\n",
        "# Trong một số mô hình chatbot, history được sử dụng để lưu giữ lịch sử của cuộc hội thoại để mô hình có thể tham chiếu lại các câu trả lời trước đây, từ đó duy trì ngữ cảnh trong suốt cuộc đối thoại.\n",
        "# Tuy nhiên, vì trong hàm predict không thấy có logic nào liên quan đến history, không thể kết luận chắc chắn về cách history được sử dụng mà không thấy được phần còn lại của code hoặc cách hàm này được gọi trong giao diện Gradio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R392mLklPAue"
      },
      "source": [
        "Xử lý tiếng việt đối với việc xài Embedding Model: multilingual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCKcuaTzqfOu"
      },
      "outputs": [],
      "source": [
        "# # Load thêm 1 con Dịch\n",
        "# from transformers import pipeline\n",
        "\n",
        "# # Initialize the translation pipeline\n",
        "# translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-vi\")\n",
        "\n",
        "# # Function to handle the query and translation\n",
        "# def query_and_translate(input_query):\n",
        "#     try:\n",
        "#         response = query_engine.query(input_query)\n",
        "#         response_text = str(response)\n",
        "#         translated_response = translator(response_text, max_length=512)\n",
        "#         # [{'translation_text': 'Tôi không chắc về dự báo thời tiết ...'}] <class 'list'>\n",
        "#         return translated_response[0]['translation_text']\n",
        "#     except Exception as e:\n",
        "#         return f\"Error: {str(e)}\"\n",
        "# # Test the query and translation function\n",
        "# query = \"Thời tiết hôm nay ở Hà Nội thế nào?\"\n",
        "# response = query_and_translate(query)\n",
        "# print(response, type(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk3ANtWTw8aO"
      },
      "outputs": [],
      "source": [
        "!pip install langid\n",
        "\n",
        "import langid\n",
        "\n",
        "# Function to detect language\n",
        "def detect_language(text):\n",
        "    lang, _ = langid.classify(text)\n",
        "    return lang\n",
        "\n",
        "# Function to handle the query with language detection\n",
        "def predict_llamaparse_with_language_detection(input_query):\n",
        "    try:\n",
        "        # Detect language\n",
        "        lang = detect_language(input_query)\n",
        "\n",
        "        # Add instruction to answer in Vietnamese if the input is in Vietnamese\n",
        "        if lang == 'vi':\n",
        "            input_query = \"TRẢ LỜI CÂU HỎI BẰNG TIẾNG VIỆT. \" + input_query\n",
        "\n",
        "        response = query_engine_llamaparser.query(input_query)\n",
        "        response_text = str(response)\n",
        "\n",
        "        return response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Test the query and language detection function\n",
        "query = \"CodeMely có bao nhiêu thành viên, fanpage của CodeMely.Group cộng đồng Dev ơi mình đi đâu thế, giới thiệu đi\"\n",
        "query = 'Xin chào'\n",
        "response = predict_llamaparse_with_language_detection(query)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECDABacwWZNM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# gradio lib: Tạo user interface\n",
        "!pip install -q gradio\n",
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(predict).launch(share=True)\n",
        "# gr.ChatInterface(predict_llamaparse_with_language_detection).launch(share=True) # Tạo 1 giao diện chat,\n",
        "\n",
        "# gr.ChatInterface(query_and_translate).launch(share=True) # Tạo 1 giao diện chat\n",
        "# Sử dụng hàm predict như một back-end để xử lý các truy vấn và sinh phản hồi.\n",
        "# launch(share=True) khởi động giao diện người dùng và tạo một liên kết có thể chia sẻ để người khác cũng có thể truy cập giao diện chat này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZw2rWfZMMjS"
      },
      "source": [
        "Bug khi xài model nặng tầm 16-17GB trở lên\n",
        "```\n",
        "OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 25.06 MiB is free. Process 5742 has 14.72 GiB memory in use. Of the allocated memory 14.35 GiB is allocated by PyTorch, and 237.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tovTxROhZUF"
      },
      "source": [
        "#### Tăng hiệu suất Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqg6rR04hUux"
      },
      "source": [
        "\n",
        "- Bắt đầu với giá trị context_window là 1024: Đây là một giá trị hợp lý cho nhiều trường hợp mà vẫn giữ được ngữ cảnh cần thiết.\n",
        "- Giữ max_new_tokens ở mức 128: Điều này giúp đảm bảo rằng câu trả lời đầy đủ mà không tốn quá nhiều tài nguyên.\n",
        "- Sử dụng chunk_size là 512: Giảm chunk_size giúp tăng tốc độ xử lý nhưng vẫn giữ được chất lượng của việc truy vấn dữ liệu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOndSISDMaZE"
      },
      "source": [
        "# Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrklVnNYIpfR"
      },
      "source": [
        "- Các vấn đề liên quan đến Toán học => Có thể sẽ cần Instruction Tuning dựa trên Prompting nhiều hơn. Khi test với 2 thành viên, anh Châu 27/11/2001, anh Tân 25/05/2001 thì nó cứ trả lời anh Châu nhiều tuổi hơn.\n",
        "- Tối ưu model LLMs, tối ưu prompting CHO TIẾNG VIỆT.\n",
        "- Model có vấn đề trong việc ghi nhớ ngữ cảnh ngắn hạn, chẳng hạn cung cấp tên và hỏi tên của mình nó không trả lời được, dường như chỉ truy vấn dựa trên database có sẵn.\n",
        "- Llama-3-8B test bị lỗi:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP7IK-9Yiz2W"
      },
      "source": [
        "Mô hình RAG (Retrieval-Augmented Generation) hoạt động bằng cách kết hợp cơ chế tìm kiếm thông tin (retrieval) với quá trình sinh ngôn ngữ tự động. Nó tìm kiếm thông tin có liên quan từ một bộ dữ liệu lớn trước khi thực hiện việc sinh ngôn ngữ. Tuy nhiên, RAG có thể không liên kết thông tin cũng như các chatbot dựa trên mô hình hóa ngôn ngữ như GPT-3 hoặc GPT-4 của OpenAI, vì nó thiết kế để trả lời từng câu hỏi riêng biệt mà không xây dựng hoặc duy trì một cuộc hội thoại có ngữ cảnh.\n",
        "\n",
        "Một số hạn chế của RAG có thể bao gồm:\n",
        "\n",
        "Bản chất Stateless: RAG thường không giữ ngữ cảnh của cuộc trò chuyện trước đó, điều này có nghĩa là mỗi câu trả lời được tạo ra mà không cần đến thông tin từ cuộc hội thoại trước.\n",
        "Tùy thuộc vào Bộ dữ liệu: RAG phụ thuộc vào bộ dữ liệu đã được thu thập trước đó. Nếu bộ dữ liệu không chứa thông tin liên quan đến ngữ cảnh cụ thể của câu hỏi, mô hình có thể không cung cấp câu trả lời chính xác.\n",
        "Khó khăn trong việc xử lý ngữ cảnh phức tạp: Khi phải đối mặt với các yêu cầu phức tạp hoặc ngữ cảnh cần sự hiểu biết sâu sắc và liên tục, RAG có thể không hiệu quả bằng các mô hình chuyên biệt về việc duy trì ngữ cảnh.\n",
        "Cập nhật thông tin: RAG không thể tự cập nhật thông tin mới nhanh chóng bằng cách học từ tương tác thực tế, điều này làm cho nó khó phản ứng với những thay đổi trong thông tin hoặc đối thoại.\n",
        "Khả năng phản hồi: Mô hình GPT-4 được thiết kế để tiếp nhận và xử lý thông tin từ câu trước để tạo ra câu sau một cách mềm dẻo hơn, cho phép nó tạo ra các cuộc đối thoại có ngữ cảnh liên tục và phong phú hơn.\n",
        "Tóm lại, mỗi loại mô hình đều có ưu và nhược điểm của riêng mình và lựa chọn sử dụng mô hình nào tùy thuộc vào yêu cầu cụ thể của ứng dụng hay dịch vụ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu-QD5o7Gjru"
      },
      "source": [
        "\n",
        "Nguyễn Thanh Châu\n",
        "https://docs.vnstock.site/functions/fundamental/#thong-tin-giao-dich-noi-bo\n",
        "\n",
        "\n",
        "Từ một câu hỏi của user (tạm thời fix trong chủ đề về thông tin giao dịch nội bộ) -> tạo truy vấn tương ứng\n",
        "và lấy ra data từ db, trả ra data\n",
        "\n",
        "\n",
        "\n",
        "[INST] You are a helpful code assistant. Your task is to generate a valid JSON object based on the given information:\n",
        "\n",
        "name: John\n",
        "lastname: Smith\n",
        "address: #1 Samuel St.\n",
        "\n",
        "Just generate the JSON object without explanations:\n",
        "[/INST]\n",
        "\n",
        "=> Trích key from user query. => truy vấn SQL.\n",
        "\n",
        "- H1: đưa csv về dạng markdown\n",
        "- H2: đưa CSV tách lấy keyword.\n",
        "- Trước khi làm bước của mình: truy vấn câu thì cho nó có cái query tách ra JSON, keyword trước.\n",
        "\n",
        "- Làm bài vnstock đi.\n",
        "\n",
        "- Thử 1 số model embedding\n",
        "\n",
        "SIMILIRATY MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAL2Mh5fiUst"
      },
      "outputs": [],
      "source": [
        "# Cài đặt thư viện vnstock từ Pypi - bản ổn định | Install vnstock from Pypi - stable version\n",
        "!pip install -U vnstock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS6Z3NNe80z3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the locale to UTF-8\n",
        "os.environ['LC_ALL'] = 'en_US.utf8'\n",
        "os.environ['LANG'] = 'en_US.utf8'\n",
        "\n",
        "# Now try to install vnstock again\n",
        "!pip install -U vnstock\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw4jglbw8eXr"
      },
      "outputs": [],
      "source": [
        "from vnstock import * #import all functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOc5UyMj6tnP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "company_insider_deals (symbol='TCB', page_size=20, page=0)\n",
        "company_insider_deals_df1 = company_insider_deals (symbol='TCB', page_size=20, page=0)\n",
        "company_insider_deals_df1 .to_csv('/data/company_insider_deals_df1', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuNYA19L8hTn"
      },
      "outputs": [],
      "source": [
        "company_insider_deals (symbol='TCB', page_size=20, page=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kggp0qt8jCa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/data/company_insider_deals_df1')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU47K1v_fMJn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTAa0PDq5vpG"
      },
      "source": [
        "Có vài cách bạn có thể thử để giảm kích thước của mô hình trong PyTorch khi sử dụng với các thư viện như Hugging Face và llama-index:\n",
        "\n",
        "1. **Quantization**: Như bạn đã nói, sử dụng quantization là một cách hiệu quả để giảm dung lượng bộ nhớ mà mô hình chiếm dụng. Bằng cách chuyển đổi dữ liệu mô hình từ float32 sang kiểu dữ liệu nhỏ hơn như int8 hoặc float16, bạn có thể giảm đáng kể kích thước mô hình.\n",
        "\n",
        "2. **Pruning**: Đây là một kỹ thuật loại bỏ các trọng số (weights) không quan trọng trong mạng neural để làm giảm kích thước mô hình và thường không ảnh hưởng nhiều đến hiệu suất mô hình. Trong PyTorch, bạn có thể sử dụng các công cụ như `torch.nn.utils.prune` để áp dụng pruning.\n",
        "\n",
        "3. **Sử dụng mô hình nhỏ hơn**: Thay vì sử dụng mô hình lớn như Llama-2-13B, bạn có thể chọn phiên bản nhỏ hơn của mô hình đó nếu có, hoặc sử dụng một kiến trúc mô hình khác có kích thước nhỏ hơn nhưng vẫn đáp ứng được yêu cầu về chất lượng.\n",
        "\n",
        "4. **Distillation**: Đây là kỹ thuật trong đó \"kiến thức\" của một mô hình lớn (teacher model) được chuyển giao sang một mô hình nhỏ hơn (student model). Kỹ thuật này giúp giữ lại hiệu suất của mô hình lớn trong khi giảm kích thước mô hình.\n",
        "\n",
        "5. **Sử dụng các tính năng tự động**: Đối với thư viện Hugging Face, bạn có thể tận dụng tính năng như `model_kwargs={\"torch_dtype\": torch.float16}` để giảm sử dụng bộ nhớ bằng cách sử dụng precision thấp hơn cho các phép tính.\n",
        "\n",
        "Lưu ý rằng mỗi kỹ thuật có thể yêu cầu một số thay đổi trong cách bạn đào tạo hoặc triển khai mô hình của mình, và việc cân bằng giữa kích thước mô hình, hiệu suất và độ chính xác là cần thiết. Có thể cần thử nghiệm để xác định phương pháp tốt nhất cho trường hợp sử dụng cụ thể của bạn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJsRB7nNzeBP"
      },
      "source": [
        "Multi-Vector Retriever for RAG on Tables + Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t1AvQ7yUars"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkEdK90lgFKn"
      },
      "source": [
        "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"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frj5LfVkgye8"
      },
      "source": [
        "tách giọng -> to text\n",
        "-> model: text to speech. Bắt được cảm xúc người đọc\n",
        "-> nhạc nền vẫn thế, giọng nói khác\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ_Fr9rOoYGD"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "003b2c20c53547f2b64cba373f5ba382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00cb1b052d5943b0893297f884ef1311": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a8c0f7546a4c248f646ccbf4460df9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108c85db35db4b4eb40fee4b731c5a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd0de5a8ef6c417ab2fce321f21a0028",
            "placeholder": "​",
            "style": "IPY_MODEL_72152e757a604f30ad54823aa61c8e79",
            "value": "model-00001-of-00004.safetensors: 100%"
          }
        },
        "158f92afc2c346ecb298854ad3e20e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97b303d8f2134494a67e032762088aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_3de89384225647f6a9a0d1eba31bd5b5",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "204dd54259b34fa78630d9cfef4a2e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21fabc2aaabd4a11b7cd2e72e3b7d273": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2543dc244515455197467a7592cba219": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d47bf7016484679b34d54fec6738013": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39bd24d84b1440409830eab68c199f85",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d12cd9f885074e9b9ad93489f01859e5",
            "value": 1
          }
        },
        "39bd24d84b1440409830eab68c199f85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c19a78d35544746b4f3d3f792ecbb54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a3ef7267abf4a73a0d2dd2c952c673a",
            "placeholder": "​",
            "style": "IPY_MODEL_5ade93d1a14c4351a4838430052c7689",
            "value": " 20.9k/20.9k [00:00&lt;00:00, 879kB/s]"
          }
        },
        "3de89384225647f6a9a0d1eba31bd5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eb406b724104ef49e74ebd7c5c1a29f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_679bc0dca08940b69e2d6db3b72dbb3e",
              "IPY_MODEL_9680b6df6ca64a07b74b52f2475c9a29",
              "IPY_MODEL_d4dea10e00e04805aa769f915d820bba"
            ],
            "layout": "IPY_MODEL_ed335d1ac3454d3baa4ff12e9f4e44a7"
          }
        },
        "3fe0c7bbeda34b2a8c03d0000fe8695c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21fabc2aaabd4a11b7cd2e72e3b7d273",
            "placeholder": "​",
            "style": "IPY_MODEL_7d72db825eba44d3bcfaf7df0bab0454",
            "value": " 5.00G/5.00G [00:32&lt;00:00, 136MB/s]"
          }
        },
        "4035e681343849068e482ec362c72271": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55dc696e88584a8dbd68b8b67ec6065b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ade93d1a14c4351a4838430052c7689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d70947f76c74722875aafd8a50f2ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62893941b58c44178816aa751abe2b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6345eaaf99d74c4d8192200f3d50a368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f5cef969e314d3b840b978ccb6836f3",
            "max": 698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2d499d208434ccab96a4deaade74383",
            "value": 698
          }
        },
        "675074d5a61e46608a772c51fa2ee8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_108c85db35db4b4eb40fee4b731c5a1e",
              "IPY_MODEL_bcb93d98abb44c208ecaa73fa96ff957",
              "IPY_MODEL_3fe0c7bbeda34b2a8c03d0000fe8695c"
            ],
            "layout": "IPY_MODEL_ea6791d5445946e683555ff90db0512d"
          }
        },
        "679bc0dca08940b69e2d6db3b72dbb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ec8e6ea19d74ee2af17e978ed52680e",
            "placeholder": "​",
            "style": "IPY_MODEL_62893941b58c44178816aa751abe2b03",
            "value": "model-00002-of-00004.safetensors:  72%"
          }
        },
        "6839f894ad094a9ebd6a52e80ad219b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "687e18bdb8a344eeacdd9a80a8f3aed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a8c0f7546a4c248f646ccbf4460df9",
            "max": 20920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca0237ce394e42eab4c0fb963ae50792",
            "value": 20920
          }
        },
        "70d4887bf47c492a927db978ac3a3573": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75738d7ac73c4634af8c140cdfc6b1a0",
            "placeholder": "​",
            "style": "IPY_MODEL_f7295d18bf134337b6617c567f280993",
            "value": " 698/698 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "72152e757a604f30ad54823aa61c8e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "724181b5f2e74a2493e0f0486c0b8d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_937f2140f1be41e79972a3c2a1267cda",
            "placeholder": "​",
            "style": "IPY_MODEL_caf82a9d37b44367bc91a54bd86c369e",
            "value": " 1/4 [00:33&lt;01:39, 33.25s/it]"
          }
        },
        "75738d7ac73c4634af8c140cdfc6b1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d7e73257854fb390bbb10154c35074": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3ef7267abf4a73a0d2dd2c952c673a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d72db825eba44d3bcfaf7df0bab0454": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ec8e6ea19d74ee2af17e978ed52680e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937229e595b1435e8a5aff2d47cf4375": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d05fc79ca755409ca1978b285b03af4e",
              "IPY_MODEL_6345eaaf99d74c4d8192200f3d50a368",
              "IPY_MODEL_70d4887bf47c492a927db978ac3a3573"
            ],
            "layout": "IPY_MODEL_b13df1cbcaca4490ac98fac7f7bd8eda"
          }
        },
        "937f2140f1be41e79972a3c2a1267cda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9680b6df6ca64a07b74b52f2475c9a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6839f894ad094a9ebd6a52e80ad219b0",
            "max": 4982953168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_003b2c20c53547f2b64cba373f5ba382",
            "value": 3575644160
          }
        },
        "97b303d8f2134494a67e032762088aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f5cef969e314d3b840b978ccb6836f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a938aeb230374e9fa8dc036e2f19b27f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13df1cbcaca4490ac98fac7f7bd8eda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcb93d98abb44c208ecaa73fa96ff957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_204dd54259b34fa78630d9cfef4a2e7b",
            "max": 4995496656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55dc696e88584a8dbd68b8b67ec6065b",
            "value": 4995496656
          }
        },
        "c289f6970d444644902daa850e8d62cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca0237ce394e42eab4c0fb963ae50792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caf82a9d37b44367bc91a54bd86c369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d05fc79ca755409ca1978b285b03af4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a938aeb230374e9fa8dc036e2f19b27f",
            "placeholder": "​",
            "style": "IPY_MODEL_2543dc244515455197467a7592cba219",
            "value": "config.json: 100%"
          }
        },
        "d12cd9f885074e9b9ad93489f01859e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4395c86ebaa4ae692825013f719ef68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4dea10e00e04805aa769f915d820bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4035e681343849068e482ec362c72271",
            "placeholder": "​",
            "style": "IPY_MODEL_5d70947f76c74722875aafd8a50f2ab6",
            "value": " 3.58G/4.98G [00:44&lt;00:40, 34.9MB/s]"
          }
        },
        "dd0de5a8ef6c417ab2fce321f21a0028": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d499d208434ccab96a4deaade74383": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e906734861ea4ce5bde63c816fc164ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d7e73257854fb390bbb10154c35074",
            "placeholder": "​",
            "style": "IPY_MODEL_c289f6970d444644902daa850e8d62cd",
            "value": "Downloading shards:  25%"
          }
        },
        "ea6791d5445946e683555ff90db0512d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed335d1ac3454d3baa4ff12e9f4e44a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ae0e10eafe43e38c303a3337fe7e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e906734861ea4ce5bde63c816fc164ef",
              "IPY_MODEL_2d47bf7016484679b34d54fec6738013",
              "IPY_MODEL_724181b5f2e74a2493e0f0486c0b8d6e"
            ],
            "layout": "IPY_MODEL_00cb1b052d5943b0893297f884ef1311"
          }
        },
        "f626731a93ab4a698ab3cf2e5a05828d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_158f92afc2c346ecb298854ad3e20e02",
              "IPY_MODEL_687e18bdb8a344eeacdd9a80a8f3aed6",
              "IPY_MODEL_3c19a78d35544746b4f3d3f792ecbb54"
            ],
            "layout": "IPY_MODEL_d4395c86ebaa4ae692825013f719ef68"
          }
        },
        "f7295d18bf134337b6617c567f280993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
